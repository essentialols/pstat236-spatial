---
title: "Areal Data Group Project"
author: "Kayla, Ingmar, Hanmo, Will"
date: "`r Sys.Date()`"
output: 
  html_document:
    keep_md: true
    toc: true
    toc_float: true
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

# Set up

**resources we are referencing:  

- http://www.css.cornell.edu/faculty/dgr2/_static/files/ov/ov_ADSA_Handout.pdf  
- Banerjee, Carlin and Gelfand, Hierarchical Modeling and Analysis for Spatial Data, 1st Edition, Ch 3  
- Bivand et al 2013 Ch 9  
- rspatial.org  

The packages we used are:  
```{r}
# install.packages(c("sp", "raster", "spdep", ....))
library(tidyverse)
library(sp)
# library(spData)
# library(raster) # or terra
library(terra)
library(spdep)
library(sf)
library(GGally)
library(ape)
## more...
```

The dataset we are using is published in `spData` and comes from: _Anselin, Luc. 1988. Spatial econometrics: methods and models. Dordrecht: Kluwer Academic, Table 12.1 p. 189._
```{r}
## load dataset(s)
columbus <- vect(system.file("shapes/columbus.shp", package="spData")[1])
df.columbus <- as.data.frame(columbus)
glimpse(df.columbus)
```

The county data is:  

* HOVAL housing value (in \$1,000)  
* INC household income (in \$1,000)  
* CRIME residential burglaries and vehicle thefts per thousand households in the neighborhood  
* OPEN open space in neighborhood  
* PLUMB percentage housing units without plumbing  

Look at some summaries of those metrics: _Kayla_   
```{r, echo=FALSE, out.width= "50%"}
## set up plotting color palette
pal = colorRampPalette(c("lightgrey", "darkblue"))(10)

cc <- cut(columbus$HOVAL, 10)
mycols <- pal[as.numeric(cc)]
hist(df.columbus$HOVAL, 
     main = "Housing value", 
     xlab = "Value  in $1000")
plot(columbus, col = mycols, main = "Housing value in $1000")

cc <- cut(columbus$INC, 10)
mycols <- pal[as.numeric(cc)]
hist(df.columbus$INC, 
     main = "Household income", 
     xlab = "Value  in $1000")
plot(columbus, col = mycols, main = "Household income in $1000") 

cc <- cut(columbus$CRIME, 10)
mycols <- pal[as.numeric(cc)]
hist(df.columbus$CRIME,
     main = "Residential burglaries and vehicle thefts", 
     xlab = "CRIME per thousand households in the neighborhood")
plot(columbus, col = mycols, main = "Residential burglaries and vehicle thefts") 

cc <- cut(columbus$OPEN, 10)
mycols <- pal[as.numeric(cc)]
hist(df.columbus$OPEN, 
     main = "Open space in neighborhood", 
     xlab = "open space")
plot(columbus, col = mycols, main = "Open space in neighborhood") 

cc <- cut(columbus$PLUMB, 10)
mycols <- pal[as.numeric(cc)]
hist(df.columbus$PLUMB, 
     main = "Percentage of housing units without plumbing", 
     xlab = "Percent no plumbing")
plot(columbus, col = mycols, main = "Percentage of housing units without plumbing") 

cc <- cut(columbus$CP, 10)
mycols <- pal[as.numeric(cc)]
hist(df.columbus$CP, 
     main = "Center vs edge (roughly urban vs rural)", 
     xlab = "Percent urban vs rural")
plot(columbus, col = mycols, main = "Center vs edge (roughly urban vs rural)") 
```

Because we plan to use housing value as our response variable try a transformation.  
```{r}
hist(log10(df.columbus$HOVAL), 
     main = "Housing value", 
     xlab = "Value  in $1000 (log_10)")
```

# Exploratory Data Analysis  

## Measures of spatial association  

### Neighbors _Kayla_  
adjacency matrix construction - nearest neighbors, categories, 1/dist  
Figures out what the neighbors are:  
```{r}
xy <- centroids(columbus)
head(neighbors <- adjacent(columbus, symmetrical=TRUE))
plot(columbus, col='lightgray', border='black', lwd=1)
p1 <- xy[neighbors[,1], ]
p2 <- xy[neighbors[,2], ]
lines(p1, p2, col='red', lwd=2)
```

As an adjecency matrix  
```{r}
head(neighbors <- adjacent(columbus, pairs = FALSE))
```

### Moran's I - _Kayla_  

Using `spdep` to test for spatial autocorrelation of the variable, by county in Columbus, OH. 

$$
I=\frac{n}{\sum_{i=1}^{n} (y_{i}-\overline{y})^{2}} \frac{\sum_{i=1}^{n} \sum_{j=1}^{n} w_{ij} (y_{i}-\overline{y}) (y_{j}-\overline{y})}{\sum_{i=1}^{n} \sum_{j=1}^{n} w_{ij}}
$$

Moran's $I$ is a global measure of spatial autocorrelation with values ranging -1 to 1. Here we are using a neighbor's matrix for any counties that are touching each other ($w_{ij}$). $n$ is the number of neighborhoods in Columbus, OH which are indexed by $i$ and $j$. $\overline{y}$ is the mean of $y_{1...n}$.    

The adjacency matrix for all counties that touch each other:  
```{r}
ww <-  adjacent(columbus, "touches", pairs=FALSE)
```

Roughly the expected value for Moran's I is $E(I)=\frac{-1}{n-1}$   
```{r}
-1/(nrow(columbus)-1)
```

Values significantly ($\alpha = 0.05$) below that indicate negative spatial autocorrelation (a phenomena that generally occurs in random datasets) and above that indicates positive spatial autocorrelation (neighbors are more similar to each other than non-neighbors). 

*** I'm seeing the formula written in 2 ways above, and with denominator's flipped  

#### House value  
```{r, out.width= "50%"}
## Moran's I
(ac <- autocor(columbus$HOVAL, ww, "moran"))
## Monte Carlo sim to test for significance (I'm following https://rspatial.org/terra/analysis/3-spauto.html#compute-morans-i)
m <- sapply(1:99, function(i) {
    autocor(sample(columbus$HOVAL), ww, "moran")
})
hist(m)

## p-value
sum(m >= ac) / 100
```
```{r, echo=FALSE}
mi <- ac
mi.p <- ""
```
So there is not significant (Moran's I = `r ac`, p > 0.05) spatial autocorrelation.

#### Household income  
```{r, out.width= "50%"}
## Moran's I
(ac <- autocor(columbus$INC, ww, "moran"))
## Monte Carlo sim to test for significance (I'm following https://rspatial.org/terra/analysis/3-spauto.html#compute-morans-i)
m <- sapply(1:99, function(i) {
    autocor(sample(columbus$INC), ww, "moran")
})
# hist(m)

## p-value
sum(m >= ac) / 100
```
```{r, echo=FALSE}
mi[2] <- ac
mi.p[2] <- "*"
```
There is significant positive (Moran's I = `r ac`, p < 0.05) Spatial autocorrelation in household income.   

#### Crime  
```{r, out.width= "50%"}
## Moran's I
(ac <- autocor(columbus$CRIME, ww, "moran"))
## Monte Carlo sim to test for significance (I'm following https://rspatial.org/terra/analysis/3-spauto.html#compute-morans-i)
m <- sapply(1:99, function(i) {
    autocor(sample(columbus$CRIME), ww, "moran")
})
# hist(m)

## p-value
sum(m >= ac) / 100
```
```{r, echo=FALSE}
mi[3] <- ac
mi.p[3] <- "*"
```
So again we see significant spatial autocorrelation (Moran's I = `r ac`, p > 0.05).  

#### Open space  
```{r, out.width= "50%"}
## Moran's I
(ac <- autocor(columbus$OPEN, ww, "moran"))
## Monte Carlo sim to test for significance (I'm following https://rspatial.org/terra/analysis/3-spauto.html#compute-morans-i)
m <- sapply(1:99, function(i) {
    autocor(sample(columbus$HOVAL), ww, "moran")
})
# hist(m)

## p-value
sum(m >= ac) / 100
```
```{r, echo=FALSE}
mi[4] <- ac
mi.p[4] <- ""
```

There is not spatial autocorrelation with open space (Moran's I = `r ac`; p-value > 0.05)

#### Plumbing  
```{r, out.width= "50%"}
## Moran's I
(ac <- autocor(columbus$PLUMB, ww, "moran"))
## Monte Carlo sim to test for significance (I'm following https://rspatial.org/terra/analysis/3-spauto.html#compute-morans-i)
m <- sapply(1:99, function(i) {
    autocor(sample(columbus$HOVAL), ww, "moran")
})
# hist(m)

## p-value
sum(m >= ac) / 100
```
```{r, echo=FALSE}
mi[5] <- ac
mi.p[5] <- "*"
```

There is significant spatial autocorrelation with plumbing (Moran's I = `r ac`, p < 0.05).

### Geary's C  __Ingmar & Kayla__  

N is the number of spatial units indexed by {\displaystyle i}i and {\displaystyle j}j; {\displaystyle x}x is the variable of interest; {\displaystyle {\bar {x}}}{\bar {x}} is the mean of {\displaystyle x}x; {\displaystyle w_{ij}}w_{ij} is a matrix of spatial weights with zeroes on the diagonal (i.e., {\displaystyle w_{ii}=0}{\displaystyle w_{ii}=0}); and {\displaystyle W}W is the sum of all {\displaystyle w_{ij}}w_{ij}.
$$
C=\frac{(n-1) \sum_{i} \sum_{j} w_{i j}\left(y_{i}-y_{j}\right)^{2}}{2\left(\sum_{i \neq j} w_{i j}\right) \sum_{i}\left(y_{i}-\bar{y}\right)^{2}}
$$

Geary's $C$ is a measure of local spatial autocorrelation that is roughly inversely related to Moran's I.  
The values ranging 0 to > 1, with values 0-1 representing positive spatial autocorrelation and values > 1 representing negative spatial autocorrelation. The neighbor's matrix ($w_{ij}$) is the same what was used for Moran's I. $\sum_{i \neq j} w_{i j}$ is the sum of that weight matrix with the diagonal equal to 0.  

#### House Value
```{r, out.width= "50%"}
(gearyc <- autocor(columbus$HOVAL, ww, "geary"))

## Monte Carlo sim to test for significance 
m <- sapply(1:99, function(i) {
    autocor(sample(columbus$HOVAL), ww, "geary")
})
hist(m)

## p-value
sum(m >= gearyc) / 100
```
```{r, echo = FALSE}
gc <- gearyc
```
No significant spatial autocorrelation (geary's c = `r gearyc`, p > 0.05).  

#### Household Income  
```{r}
(gearyc <- autocor(columbus$INC, ww, "geary"))

## Monte Carlo sim to test for significance 
m <- sapply(1:99, function(i) {
    autocor(sample(columbus$INC), ww, "geary")
})
# hist(m)

## p-value
sum(m >= gearyc) / 100
```
```{r, echo = FALSE}
gc[2] <- gearyc
```
No significant spatial autocorrelation (geary's c = `r gearyc`, p > 0.05).  

#### Crime  
```{r}
(gearyc <- autocor(columbus$CRIME, ww, "geary"))

## Monte Carlo sim to test for significance 
m <- sapply(1:99, function(i) {
    autocor(sample(columbus$CRIME), ww, "geary")
})
hist(m)

## p-value
sum(m >= gearyc) / 100
```
```{r, echo = FALSE}
gc[3] <- gearyc
```
No significant spatial autocorrelation (geary's c = `r gearyc`, p > 0.05).  

#### Open space  
```{r}
(gearyc <- autocor(columbus$OPEN, ww, "geary"))


## Monte Carlo sim to test for significance 
m <- sapply(1:99, function(i) {
    autocor(sample(columbus$OPEN), ww, "geary")
})
# hist(m)

## p-value
sum(m >= gearyc) / 100
```
```{r, echo = FALSE}
gc[4] <- gearyc
```
No significant spatial autocorrelation (geary's c = `r gearyc`, p > 0.05).  

#### Plumbing  
```{r}
(gearyc <- autocor(columbus$PLUMB, ww, "geary"))

## Monte Carlo sim to test for significance 
m <- sapply(1:99, function(i) {
    autocor(sample(columbus$PLUMB), ww, "geary")
})
# hist(m)

## p-value
sum(m >= gearyc) / 100
```
```{r, echo = FALSE}
gc[5] <- gearyc
```
No significant spatial autocorrelation (geary's c = `r gearyc`, p > 0.05).  

### Compare Moran's I and Geary's C  
Reinhard Furrer (http://user.math.uzh.ch/furrer/download/sta330/script_sta330.pdf, Version May 26, 2021) suggests to take 1-C to compare it to Moran's I more easily.  
```{r, echo=FALSE}
m <- data.frame(Morans_I = mi,
                signif = mi.p,
                Gearys_C = gc,
                One_minus_Gearys_C = 1-gc,
                signif = rep("", ),
                row.names = c("House value", "Income", "Crime", "Open space", "Plumbing"))
# m
knitr::kable(m, format = 'latex', digits = 3)
```

There are differences observed in local and global spatial autocorrelation in the data. For both Moran's I and Geary's C there was not significant spatial autocorrelation in house value or Open space. There was significant positive global spatial autocorrelation in household income, crime, and plumbing using Moran's I, but no significant local spatial autocorrelation.  

# Spatial regression models

## Constant means _Ingmar_

We decide to model the \textit{HOVAL} (home value) variable for all of our models. The simplest, yet naive, model is the zero means model, which is essentially an intercept-only model, i.e. the average of the \textit{HOVAL} variable.

$$
Y_i=\mu+\varepsilon_i
$$
Where $Y_i$ is the value of a home in 1000s of dollars, $\mu$ is the mean home value and $\epsilon_i$ are the individual deviations from the mean, which we assume to be be i.i.d. distributed.

$$
\hat{Y_i} = \bar{Y} = \frac{1}{n}\sum_{i=1}^{n}Y_i
$$

We estimate the model using \texttt{lm} function:

```{r}
# make spatial vector to simple feature
columbus.sf <- sf::st_as_sf(columbus)
zero.means <- lm(HOVAL ~ 1, data=columbus.sf)
summary(zero.means)
```

We find the average home value is \$38436. Since we will use the log-transformed home value for subsequent models, we repeat the zero means model for the transformed variable for purposes of comparison.

```{r}
# estimate log-transformed model
zero.means.log <- lm(log(HOVAL) ~ 1, data=columbus.sf)
summary(zero.means.log)
```

We find that the model estimates \textit{log(HOVAL)} to be 3.55231, which is \$34894 after exponentiating.

## Linear Model with Independent Residuals _Ingmar_

Improving over the zero means model, we model the home value \textit{HOVAL} as a linear function of its (non-spatial) covariates with i.i.d. errors.

$$
\mathbf{Y} = \mathbf{X}\beta + \varepsilon
$$

where X is a matrix of the predictors \textit{INC} (income), \textit{CRIME}, \textit{OPEN} (open space in neighborhood), and \textit{CP} (whether the neighborhood is in the center or periphery). 

We estimate the model with

```{r}
col.lm <- lm(HOVAL~INC+CRIME+OPEN, data=columbus.sf)
summary(col.lm)
```

```{r}
par(mfrow=c(2,2))
plot(col.lm)
```

The diagnostic plots suggests that the assumptions of regression are not satisfied, which is to be expected since we know that most areal data is spatially dependent.

## Simultaneous Autoregressive Models  

### SAR error model _Ingmar_

Use global moran's i to test model residuals for spatial correlation
lm.morantest()

To address the shortcomings of the linear model under the assumption of i.i.d. errors, we introduce Simultaneous Autoregressive Models. The models solve simultaneously for the regression coefficients and for the autoregressive error structure. In the Spatial Error Model, spatial autocorrelation enters in the specification only through the error terms.

To derive the model, we formulate the error as a first-order spatial autoregressive process

\begin{equation}
\varepsilon=\lambda W \varepsilon+u
\end{equation}

where $\lambda$ is the autoregressive parameter, $W$ is the row-standardised spatial weights matrix $W$ (that is, the weights are standardised such that $\Sigma_j W_{ij} = 1 \quad\text{for all}\quad i$), and $u_{i}$ a random error term, assumed to be i.i.d. If $|\lambda|<1$ and solving for $\varepsilon$ yields

\begin{equation}
\varepsilon=(I-\lambda W)^{-1} u
\end{equation}

We obtain the spatial error model by inserting $\varepsilon$ into the standard regression model

\begin{equation}
y=X \beta+(I-\lambda W)^{-1} u
\end{equation}

where X is a matrix of the covariates $INC$, $CRIME$, $OPEN$, and $CP$. with $E\left[u u^{\prime}\right]=\sigma^{2} I$, which results in the following error variance-covariance matrix

\begin{equation}
E\left[\varepsilon \varepsilon^{\prime}\right]=\sigma^{2}(I-\lambda W)^{-1}\left(I-\lambda W^{\prime}\right)^{-1}
\begin{equation}

In order to estimate this model, we first create a list of spatial weights for neighbors. Then we estimate the model using the errorsarlm function from the spatialreg package.

```{r}
# make simple feature to neighborhood
columbus.nb <- poly2nb(columbus.sf)

# make neighborhood to list of weights
lw <- nb2listw(columbus.nb, style="W")

# estimate error SAR model without transformation
col.errW.eig <- errorsarlm(HOVAL~INC+CRIME+OPEN+CP, data=columbus.sf,
 lw, method="eigen", quiet=T)

# look at the residuals
hist(residuals(col.errW.eig))
```

The residuals are not normally distributed. We therefore, log-transform the response to normalize them.

```{r}
# estimate error SAR model with log transformation
col.errW.eig.log <- errorsarlm(log(HOVAL)~INC+CRIME+OPEN+CP, data=columbus.sf,
 lw, method="eigen", quiet=T)

hist(residuals(col.errW.eig.log))

# print model summary
summary(col.errW.eig.log, correlation=TRUE)
```

We find that only $CRIME$ and $OPEN$ are significant at the $\alpha=0.05$ level. For every additional major theft (residential burglaries and vehicle theft) per 1000 households, the predicted home value decreases by approximately \$1010 ($e^{0.0099247}=1.009974$), holding other variables constant. For every additional unit (which is not provided in the description) of open space, the home value is expected to increase by \$1020 ($e^{0.0194551}=1.019646$), holding other variables constant.

We also check whether there is some spatial dependence within the residuals of the model by performing Moran's I test. The test tests $H_0$: There is no spatial dependence against $H_1$: There is spatial dependence. We reject $H_0$ if the test statistic $ <\alpha=0.05$.

```{r}
# Run Moran's I test for col.errW.eig.log
moran.test(residuals(col.errW.eig.log), lw) # not significant
```

With a p-value of 0.3559, we fail to reject $H_0$. There does not seem to be any spatial dependence present in the residuals of our error model.

### SAR lag and Durbin models _Will_

Next, we compute the SAR lag and SAR Durbin models on the log-transformed housing value
with the same covariates used for the SAR error model detailed above.

The SAR lag model assumes the form
$$
y = \rho W y + X\beta + \varepsilon,
$$
where $W$ is the neighbor matrix and $\rho$ is a real parameter that further controls the
interaction strength between teh data. Like the SAR error model, the SAR lag model is a global
spatial model since it is first-order autoregressive. In this model, the autoregressive structure
comes from the data $y$ rather than from the residuals.

```{r}
# make simple feature to neighborhood
columbus.nb <- poly2nb(columbus.sf)

# make neighborhood to list of weights
lw <- nb2listw(columbus.nb, style="W")

# estimate error SAR model without transformation
lag <- lagsarlm(HOVAL~INC+CRIME+OPEN+CP, data=columbus.sf,
 lw, Durbin=FALSE)

# look at the residuals
hist(residuals(lag))
```


As before, we see that the residuals are not normally distributed, so we log-transform HOVAL.


```{r}
# estimate error SAR model with log transformation
lag.log <- lagsarlm(log(HOVAL)~INC+CRIME+OPEN+CP, data=columbus.sf,
 lw, Durbin = FALSE)

hist(residuals(lag.log))

# print model summary
summary(lag.log, correlation=TRUE)
```

As with the error model, the CRIME and OPEN covariates have the lowest p-values.
However for this model, only OPEN
falls below the threshold p-value of 0.05, whereas CRIME gave the lowest p-value in
the SAR error model.
The p-values are higher than those in the error model for all covariates except OPEN.,
indicating less spatial correlation overall.

Finally, we test for spatial dependence among the residuals.

```{r}
# Run Moran's I test for col.errW.eig.log
moran.test(residuals(lag.log), lw) # not significant
```

The p-value is 0.05942, we cannot conclude that there is residual spatial dependence.



The spatial Durbin model (SDM) takes the form
$$
y = \rho Wy + WX\theta + X\beta + \varepsilon.
$$
This model generalizes the SAR lag model by the $WX\theta$ term which introduces local spatial
dependence to the model.

```{r}
# estimate SAR Durbin model with log transformation
durbin.log <- lagsarlm(formula=hoval.log,data=columbus.sf,listw=lw,Durbin=TRUE)

hist(residuals(durbin.log))
```

We use this model on the log-transformed housing data as before:


```{r}
# estimate Durbin model with log transformation
durbin.log <- lagsarlm(log(HOVAL)~INC+CRIME+OPEN+CP, data=columbus.sf,
 lw, Durbin = TRUE)

hist(residuals(durbin.log))

# print model summary
summary(durbin.log, correlation=TRUE)
```
In this model, the covariates CP,CRIME and now also OPEN have significant p-values, and overall
the p-values are lower in this model than the other SAR models considered. Since the Durbin model
is the only SAR model we've considered that includes local spatial dependence of the data, it is
possible that the local dependence is more significant, or at least that both local and global
effects are necessary to describe the spatial dependence.

A final remark on SAR models: we can further generalize the Durbin model to include autoregressive
residuals as in the error model. The resulting model generalizes each of the 3 SAR models we have
analyzed and assumed the form

$$
y = \rho Wy + WX\theta + X\beta + (I - \lambda W)^{-1}u.
$$

While this is the richest SAR model, it also includes the largest number of parameters, making it
more costly to evaluate and potentially more difficult to interpret.

### Likelihood Ratio _Will_
We compute a likelihood ratio to compare the lag and Durbin models on the log(HOVAL) data
(I don't know why library(lmtest) is not found when I run the next block. Does anyone else encounter this problem?)

```{r}
library(lmtest)
lrtest(lag.log, durbin.log)
```

## Conditional Autoregressive Models  _Hanmo_

In the next step, we implement the conditional autoregressive (CAR) model on the Columbus data to study the impact of covariates on House values in Columbus, OH, 1980 with spatial information. 

The CAR model essentially assumes the spatial estimation is conditional on the value of neighbors. As a typical Bayes model, CAR model assumes prior distribution on the model parameters and applies computationally intensive sampling techniques like Markov Chain Monte Carlo (MCMC) or MCMC with Gibbs sampling to find the fitted parameters.



Since we have one response variable *House value*, the CAR model has only one random effect $\phi_k$ at each spatial location $k$. Because *House value* is continuous, Gaussian distribution is preferred for the CAR model and we take logarithm of the *House value* as well to make it normally distributed.


The package *CARBayes* by Duncan Lee is used to apply the CAR model in R. The specific function we use is *S.CARleroux*, where it specifies a CAR model proposed by Brian G. Leroux in 2000. The model expression is as follows. 

* $\mathbf{W}$ is a 0-1 neighborhood or weight matrix
* $k = 1, \dots, K$ is the index for a certain areal unit
* $\rho$ is a spatial correlation parameter, with $\rho=0$ means indenpendence and $\rho=1$ indicates strong spatial correlation


$$
\psi_{k}=\phi_{k}
$$
$$
\phi_{k} \mid \phi_{-k}, \mathbf{W}, \tau^{2}, \rho \sim \mathrm{N}\left(\frac{\rho \sum_{i=1}^{K} w_{k i} \phi_{i}}{\rho \sum_{i=1}^{K} w_{k i}+1-\rho}, \frac{\tau^{2}}{\rho \sum_{i=1}^{K} w_{k i}+1-\rho}\right)
$$

$$
\begin{aligned}
\tau^{2} & \sim \operatorname{Inverse}-\operatorname{Gamma}(a, b) \\
\rho & \sim \text { Uniform }(0,1)
\end{aligned}
$$


```{r echo=F,cache=T}
df.columbus_CAR = df.columbus
df.columbus_CAR$CP = as.factor(df.columbus_CAR$CP)
ggpairs(data = df.columbus_CAR, columns = c(7, 8:12, 18))
```


```{r cache=T}
set.seed(2021)
car_model_gaussian = CARBayes::S.CARleroux(log(HOVAL)~INC+CRIME+OPEN+DISCBD+CP, data = df.columbus_CAR, family = "gaussian", W = ww, burnin = 100000, n.sample = 300000, thin = 10, verbose = F)
```

```{r echo=F}
# print(paste0("The log likehood is ", round(logLik(car_model_gaussian), 2)))
# INC+CRIME+DISCBD+X+Y+PERIMETER+CP 13.59
```



```{r echo=F}
betas1 <- CARBayes::summarise.samples(car_model_gaussian$samples$beta, quantiles=c(0.5, 0.025, 0.975))
resultsCAR1 <- betas1$quantiles
rownames(resultsCAR1) <- colnames(car_model_gaussian$X)
knitr::kable(resultsCAR1, caption = "95% confidence intervals for model coefficients")

# print(paste0("The MSE is ", round(sqrt(sum((exp(car_model_gaussian$fitted.values) - df.columbus_CAR$HOVAL)^2)), 2)))

plot(x = df.columbus_CAR$HOVAL, y = exp(car_model_gaussian$fitted.values), type="p",
     xlab = "housing value", ylab = "Fitted value", main = "Fitted value vs Observed value")
abline(0,1)
```

The log likelihood of this model is 17.97  while the training root mean square error (RMSE) is 59.46. From the table of $95\%$ confidence intervals for coefficients, we have

* *CRIME* and *PLUMB* are the two variables that is significant since their confidence intervals don't have zero involved.
* Hold other predictors fixed, regions with **less crimes** tend to have higher *House values*.
* Hold other predictors fixed, regions with **higher household incomes** tend to have higher *House values*.
* Hold other predictors fixed, regions with **more open area** tend to have higher *House values*.
* Hold other predictors fixed, regions with **closer distance to CBD** tend to have higher *House values*.
* Hold other predictors fixed, **core** regions tend to have higher *House values*.

```{r echo=F}
cc <- cut(residuals(car_model_gaussian), 10)
mycols <- pal[as.numeric(cc)]
plot(columbus, col = mycols, main = "Residuals")
```

```{r}
# Moran I
Moran.I(residuals(car_model_gaussian), ww)$p.value
```

We also tested the Moran's I autocorrelation coefficient for the residuals. It shows there is **no** significant spatial correlation of the residuals since the p-value $p = 0.1 > 0.05$. Therefore, our CAR model fits the areal data nicely and leaves no significant spatial information in the residuals.
 

# Conclusions  

# References  

## Packages used  

```{r}
citation("tidyverse")
citation("sp")
citation("spData")
citation("terra")
citation("spdep")
```

## Other resources:  
```{r}
## rspataial, cornell thingy, 2 book chapters and paper Wendy shared
bibentry(
   bibtype = "Manual",
   title = "R Spatial",
   author = person("R Core Team"),
   organization = "R Foundation for Statistical Computing",
   address = "Vienna, Austria",
   year = 2013,
   url = "https://www.R-project.org/",
   key = "R")
```

